"""
Comparison endpoints router
Handles topic comparisons, section comparisons, and similarity searches
"""
from fastapi import APIRouter, Depends, HTTPException, status, Path
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy import text
import logging
import json

from app.schemas.comparison import (
    ComparisonRequest,
    ComparisonResponse,
    SectionComparisonRequest,
    SectionComparisonResponse,
    SimilarSectionsRequest,
    SimilarSectionsResponse,
    SectionDetail,
    SimilarSection
)
from app.services.rag_service import get_rag_service
from app.services.groq_service import get_groq_service
from app.services.qdrant_service import get_qdrant_service
from app.services.voyage_service import get_voyage_service
from app.db.database import get_db

# Configure logging
logger = logging.getLogger(__name__)

router = APIRouter()


@router.post(
    "/compare",
    response_model=ComparisonResponse,
    summary="Compare topic across all standards",
    description="""
    Compare how PMBOK, PRINCE2, and ISO 21502 address a specific topic.

    Returns:
    - LLM-generated comparison analysis (Similarities, Differences, Unique Elements)
    - Source references from each standard
    - Token usage statistics

    Example topics: "Risk Management", "Stakeholder Engagement", "Quality Assurance"
    """,
    response_description="Comprehensive cross-standard comparison"
)
async def compare_topic(
    request: ComparisonRequest,
    db: Session = Depends(get_db)
):
    """
    Compare how all three standards address a specific topic.

    This endpoint:
    1. Searches for relevant sections in each standard
    2. Sends context to LLM for comparative analysis
    3. Returns structured comparison with sources
    """
    try:
        logger.info(f"Topic comparison request: '{request.topic}'")

        # Get RAG service
        rag_service = get_rag_service()

        # Perform comparison
        result = rag_service.compare_standards(
            topic=request.topic,
            db_session=db,
            top_k_per_standard=request.top_k_per_standard,
            score_threshold=request.score_threshold
        )

        logger.info(f"Comparison completed for topic: '{request.topic}'")
        return result

    except ValueError as e:
        logger.error(f"Validation error in comparison: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid request: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Comparison failed for topic '{request.topic}': {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Comparison operation failed. Please try again later."
        )


@router.post(
    "/compare/stream",
    summary="Stream topic comparison across all standards",
    description="""
    Stream comparison analysis as it's being generated by the LLM.

    Returns:
    - Server-Sent Events (SSE) stream with analysis chunks
    - Final event contains complete metadata and sources

    The stream includes:
    1. Initial metadata event with sources
    2. Text chunks as they're generated
    3. Final completion event
    """,
    response_description="Streaming comparison with Server-Sent Events"
)
async def compare_topic_stream(
    request: ComparisonRequest,
    db: Session = Depends(get_db)
):
    """
    Stream comparison analysis across standards using Server-Sent Events.
    """
    async def generate():
        try:
            logger.info(f"Streaming topic comparison request: '{request.topic}'")

            # Get services
            voyage_service = get_voyage_service()
            qdrant_service = get_qdrant_service()
            groq_service = get_groq_service()

            # Embed the topic
            topic_embedding = voyage_service.embed_query(request.topic)

            # Search each standard
            standards = ["PMBOK", "PRINCE2", "ISO_21502"]
            all_results = {}

            for standard in standards:
                results = qdrant_service.search_by_standard(
                    query_vector=topic_embedding,
                    standard=standard,
                    limit=request.top_k_per_standard,
                    score_threshold=request.score_threshold
                )
                all_results[standard] = results

            # Fetch metadata from database
            chunk_data = {}
            for standard, results in all_results.items():
                if not results:
                    chunk_data[standard] = []
                    continue

                chunk_ids = [str(result['id']) for result in results]
                scores = {str(result['id']): result['score'] for result in results}

                query = text("""
                    SELECT
                        id::text as id,
                        standard::text,
                        section_number,
                        section_title,
                        page_start,
                        page_end,
                        content_cleaned as content,
                        citation_key
                    FROM document_sections
                    WHERE id::text = ANY(:ids)
                    ORDER BY array_position(:ids, id::text)
                """)

                rows = db.execute(query, {"ids": chunk_ids}).fetchall()

                chunks = []
                for row in rows:
                    chunk = dict(row._mapping)
                    chunk['score'] = scores.get(chunk['id'], 0.0)
                    chunks.append(chunk)

                chunk_data[standard] = chunks

            # Format sources for frontend
            year_map = {'PMBOK': '2021', 'PRINCE2': '2017', 'ISO_21502': '2020'}
            sources = {}

            for standard in standards:
                sources[standard] = []
                for chunk in chunk_data[standard]:
                    std = chunk['standard']
                    year = year_map.get(std, '2021')
                    page_ref = f"p. {chunk['page_start']}"
                    if chunk.get('page_end') and chunk['page_end'] != chunk['page_start']:
                        page_ref = f"pp. {chunk['page_start']}-{chunk['page_end']}"

                    citation = f"{std} ({year}), Section {chunk['section_number']}, {page_ref}"
                    content_preview = chunk['content'][:200] + '...' if len(chunk['content']) > 200 else chunk['content']

                    sources[standard].append({
                        'section_number': chunk['section_number'],
                        'section_title': chunk['section_title'],
                        'page_start': chunk['page_start'],
                        'page_end': chunk.get('page_end'),
                        'citation': citation,
                        'relevance_score': chunk['score'],
                        'content_preview': content_preview
                    })

            # Send initial metadata with sources
            metadata_event = {
                'type': 'metadata',
                'topic': request.topic,
                'sources': sources
            }
            yield f"data: {json.dumps(metadata_event)}\n\n"

            # Build prompt
            system_prompt = groq_service._build_comparison_system_prompt()
            user_prompt = groq_service._build_comparison_user_prompt(
                topic=request.topic,
                pmbok_chunks=chunk_data["PMBOK"],
                prince2_chunks=chunk_data["PRINCE2"],
                iso_chunks=chunk_data["ISO_21502"]
            )

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            # Stream the comparison analysis
            for chunk in groq_service.generate_response_stream(
                messages=messages,
                temperature=0.3,
                max_tokens=3072
            ):
                chunk_event = {
                    'type': 'chunk',
                    'content': chunk
                }
                yield f"data: {json.dumps(chunk_event)}\n\n"

            # Send completion event
            completion_event = {
                'type': 'done'
            }
            yield f"data: {json.dumps(completion_event)}\n\n"

            logger.info(f"Streaming comparison completed for topic: '{request.topic}'")

        except Exception as e:
            logger.error(f"Streaming comparison failed for topic '{request.topic}': {e}")
            error_event = {
                'type': 'error',
                'message': str(e)
            }
            yield f"data: {json.dumps(error_event)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"
        }
    )


@router.post(
    "/sections-by-topic",
    summary="Get top sections by topic for side-by-side comparison",
    description="""
    Search for the most relevant section from each standard for a given topic.

    Returns:
    - Top 1 section from PMBOK
    - Top 1 section from PRINCE2
    - Top 1 section from ISO 21502

    Use this for side-by-side section comparison without AI analysis.
    """,
    response_description="Top sections from each standard"
)
async def get_sections_by_topic(
    request: ComparisonRequest,
    db: Session = Depends(get_db)
):
    """
    Get the most relevant section from each standard for a topic.

    This endpoint returns full section content for side-by-side comparison.
    """
    try:
        logger.info(f"Section search request for topic: '{request.topic}'")

        # Get services
        voyage_service = get_voyage_service()
        qdrant_service = get_qdrant_service()

        # Embed the topic
        topic_embedding = voyage_service.embed_query(request.topic)

        # Search each standard (top 1 only)
        standards = ["PMBOK", "PRINCE2", "ISO_21502"]
        all_sections = {}

        for standard in standards:
            results = qdrant_service.search_by_standard(
                query_vector=topic_embedding,
                standard=standard,
                limit=1,  # Top 1 section only
                score_threshold=request.score_threshold
            )

            if not results:
                all_sections[standard] = None
                continue

            # Fetch full section data
            section_id = str(results[0]['id'])
            relevance_score = results[0]['score']

            query = text("""
                SELECT
                    id::text,
                    standard::text,
                    section_number,
                    section_title,
                    page_start,
                    page_end,
                    content_cleaned as content,
                    citation_key
                FROM document_sections
                WHERE id::text = :section_id
            """)

            row = db.execute(query, {"section_id": section_id}).fetchone()

            if row:
                year_map = {'PMBOK': '2021', 'PRINCE2': '2017', 'ISO_21502': '2020'}
                std = row[1]
                year = year_map.get(std, '2021')
                page_ref = f"p. {row[4]}"
                if row[5] and row[5] != row[4]:
                    page_ref = f"pp. {row[4]}-{row[5]}"

                citation = f"{std} ({year}), Section {row[2]}, {page_ref}"

                all_sections[standard] = {
                    "id": row[0],
                    "standard": std,
                    "section_number": row[2],
                    "section_title": row[3],
                    "page_start": row[4],
                    "page_end": row[5],
                    "content": row[6],
                    "citation": citation,
                    "relevance_score": relevance_score
                }
            else:
                all_sections[standard] = None

        logger.info(f"Section search completed for topic: '{request.topic}'")

        return {
            "topic": request.topic,
            "sections": all_sections
        }

    except Exception as e:
        logger.error(f"Section search failed for topic '{request.topic}': {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Section search failed. Please try again later."
        )


@router.post(
    "/compare-sections",
    response_model=SectionComparisonResponse,
    summary="Compare specific sections directly",
    description="""
    Perform direct comparison between 2-3 specific sections.

    Returns:
    - Section details for each section
    - LLM-generated comparative analysis
    - Token usage statistics

    Use this for deep-dive comparison of specific sections you've identified.
    """,
    response_description="Direct section-to-section comparison"
)
async def compare_sections(
    request: SectionComparisonRequest,
    db: Session = Depends(get_db)
):
    """
    Compare specific sections from different (or same) standards.

    This endpoint allows precise comparison of sections identified by UUID.
    """
    try:
        logger.info(f"Section comparison request: {len(request.section_ids)} sections")

        # Fetch section details
        query = text("""
            SELECT
                id::text,
                standard::text,
                section_number,
                section_title,
                page_start,
                page_end,
                content_cleaned as content,
                citation_key
            FROM document_sections
            WHERE id::text = ANY(:ids)
        """)

        rows = db.execute(query, {"ids": request.section_ids}).fetchall()

        if len(rows) != len(request.section_ids):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"One or more sections not found"
            )

        # Format sections
        year_map = {'PMBOK': '2021', 'PRINCE2': '2017', 'ISO_21502': '2020'}

        sections = []
        for row in rows:
            standard = row[1]
            year = year_map.get(standard, '2021')
            page_ref = f"p. {row[4]}"
            if row[5] and row[5] != row[4]:
                page_ref = f"pp. {row[4]}-{row[5]}"

            citation = f"{standard} ({year}), Section {row[2]}, {page_ref}"

            sections.append({
                "id": row[0],
                "standard": standard,
                "section_number": row[2],
                "section_title": row[3],
                "page_start": row[4],
                "page_end": row[5],
                "content": row[6],
                "citation": citation
            })

        # Generate comparison using LLM
        groq_service = get_groq_service()

        # Build prompt for section comparison
        prompt_parts = ["Compare the following sections:\n"]
        for i, section in enumerate(sections, 1):
            prompt_parts.append(f"\n### Section {i}: {section['standard']} - {section['section_title']}")
            prompt_parts.append(f"Citation: {section['citation']}")
            prompt_parts.append(f"Content: {section['content']}\n")

        prompt_parts.append("""
Provide a detailed comparison addressing:
1. **Main Purpose**: What each section aims to accomplish
2. **Key Concepts**: Important concepts or definitions covered
3. **Similarities**: Common themes, approaches, or guidance
4. **Differences**: Unique perspectives, methodologies, or emphasis
5. **Practical Implications**: How these differences might affect practice

Be specific and cite the sections by name when making comparisons.""")

        messages = [
            {"role": "system", "content": "You are an expert in comparing project management standards. Provide detailed, analytical comparisons."},
            {"role": "user", "content": "".join(prompt_parts)}
        ]

        llm_response = groq_service.generate_response(
            messages=messages,
            temperature=0.3,
            max_tokens=2048
        )

        logger.info(f"Section comparison completed for {len(sections)} sections")

        return {
            "sections": sections,
            "analysis": llm_response['content'],
            "usage_stats": {
                "model": llm_response['model'],
                "tokens": llm_response['usage']
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Section comparison failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Section comparison failed. Please try again later."
        )


@router.get(
    "/similarities/{section_id}",
    response_model=SimilarSectionsResponse,
    summary="Find similar sections across standards",
    description="""
    Find sections semantically similar to a given section.

    Returns:
    - Source section details
    - List of similar sections with similarity scores
    - Can filter to exclude/include same standard

    Useful for discovering related concepts across standards.
    """,
    response_description="Similar sections ranked by similarity"
)
async def find_similar_sections(
    section_id: str = Path(..., description="Section UUID to find similarities for"),
    limit: int = 10,
    score_threshold: float = 0.5,
    include_same_standard: bool = False,
    db: Session = Depends(get_db)
):
    """
    Find sections similar to a given section using vector similarity.

    This endpoint uses the stored embeddings to find semantically similar content.
    """
    try:
        logger.info(f"Finding similar sections for: {section_id}")

        # Get source section
        source_query = text("""
            SELECT
                id::text,
                standard::text,
                section_number,
                section_title,
                page_start,
                page_end,
                content_cleaned as content,
                embedding
            FROM document_sections
            WHERE id::text = :section_id
        """)

        source_row = db.execute(source_query, {"section_id": section_id}).fetchone()

        if not source_row:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Section '{section_id}' not found"
            )

        source_standard = source_row[1]
        source_embedding = source_row[7]

        if not source_embedding:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Section does not have an embedding"
            )

        # Convert embedding from pgvector string format to list if needed
        if isinstance(source_embedding, str):
            # Remove brackets and parse as floats
            source_embedding = [float(x) for x in source_embedding.strip('[]').split(',')]

        # Search for similar sections in Qdrant
        qdrant_service = get_qdrant_service()

        # Search with higher limit if excluding same standard
        search_limit = limit * 2 if not include_same_standard else limit

        search_results = qdrant_service.search(
            query_vector=source_embedding,
            limit=search_limit + 1,  # +1 because source will be in results
            score_threshold=score_threshold
        )

        # Filter results
        similar_sections = []
        for result in search_results:
            result_id = str(result['id'])

            # Skip the source section itself
            if result_id == section_id:
                continue

            # Skip same standard if requested
            if not include_same_standard and result['payload'].get('standard') == source_standard:
                continue

            similar_sections.append(result)

            if len(similar_sections) >= limit:
                break

        # Fetch full metadata for similar sections
        if similar_sections:
            similar_ids = [str(r['id']) for r in similar_sections]
            scores = {str(r['id']): r['score'] for r in similar_sections}

            similar_query = text("""
                SELECT
                    id::text,
                    standard::text,
                    section_number,
                    section_title,
                    page_start,
                    page_end,
                    content_cleaned as content
                FROM document_sections
                WHERE id::text = ANY(:ids)
            """)

            similar_rows = db.execute(similar_query, {"ids": similar_ids}).fetchall()

            # Format results
            year_map = {'PMBOK': '2021', 'PRINCE2': '2017', 'ISO_21502': '2020'}
            formatted_similar = []

            for row in similar_rows:
                section_id_str = row[0]
                standard = row[1]
                year = year_map.get(standard, '2021')
                page_ref = f"p. {row[4]}"
                if row[5] and row[5] != row[4]:
                    page_ref = f"pp. {row[4]}-{row[5]}"

                citation = f"{standard} ({year}), Section {row[2]}, {page_ref}"
                content_preview = row[6][:200] + "..." if len(row[6]) > 200 else row[6]

                formatted_similar.append({
                    "id": section_id_str,
                    "standard": standard,
                    "section_number": row[2],
                    "section_title": row[3],
                    "page_start": row[4],
                    "citation": citation,
                    "similarity_score": scores.get(section_id_str, 0.0),
                    "content_preview": content_preview
                })
        else:
            formatted_similar = []

        # Format source section
        year_map = {'PMBOK': '2021', 'PRINCE2': '2017', 'ISO_21502': '2020'}
        source_year = year_map.get(source_standard, '2021')
        source_page_ref = f"p. {source_row[4]}"
        if source_row[5] and source_row[5] != source_row[4]:
            source_page_ref = f"pp. {source_row[4]}-{source_row[5]}"

        source_citation = f"{source_standard} ({source_year}), Section {source_row[2]}, {source_page_ref}"

        source_section = {
            "id": source_row[0],
            "standard": source_standard,
            "section_number": source_row[2],
            "section_title": source_row[3],
            "page_start": source_row[4],
            "page_end": source_row[5],
            "content": source_row[6],
            "citation": source_citation
        }

        logger.info(f"Found {len(formatted_similar)} similar sections for {section_id}")

        return {
            "source_section": source_section,
            "similar_sections": formatted_similar,
            "total_found": len(formatted_similar)
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Similarity search failed for {section_id}: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Similarity search failed. Please try again later."
        )